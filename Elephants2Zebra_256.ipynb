{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38564bittf240rc1conda4488f2b542be4118819027353a312fb3",
   "display_name": "Python 3.8.5 64-bit ('tf_2.4.0-rc1': conda)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.train import Checkpoint\n",
    "from tensorflow_addons.layers import InstanceNormalization \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU, Activation, BatchNormalization, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset from pre defined tensorflow_dataset module (monet2photo)\n",
    "data = tfds.load('cycle_gan/horse2zebra', as_supervised=True)\n",
    "\n",
    "# assigning the monet to x and photo to y \n",
    "train_zebras = data['trainB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zebras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elephant_dir = '../DATA/elephants/'\n",
    "train_elephants = tf.data.Dataset.list_files(Elephant_dir + '*', shuffle=True)\n",
    "\n",
    "def parse_image(filename):\n",
    "  parts = tf.strings.split(filename, os.sep)\n",
    "  label = parts[-2]\n",
    "\n",
    "  image = tf.io.read_file(filename)\n",
    "  image = tf.image.decode_jpeg(image, channels=3)\n",
    "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "  image = tf.image.resize(image, [256, 256])\n",
    "  return image, label\n",
    "\n",
    "train_elephants = train_elephants.map(parse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring the constants\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# used to prevent identity loss i.e. loss in the color of picture when transforming\n",
    "LAMBDA = 10\n",
    "\n",
    "# number of iteration to perform the training \n",
    "EPOCHS = 300\n",
    "\n",
    "# used to shuffle 1000 images\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "# declaring batches to train on a number of specified images simultaneously\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "# image dimensions\n",
    "IMG_WIDTH = 256\n",
    "IMG_HEIGHT = 256\n",
    "CHANNELS = 3\n",
    "\n",
    "# setting random normalized weights \n",
    "weight_initializer = RandomNormal(stddev=0.02)\n",
    "\n",
    "# the loss function used here is BinaryCrossentropy as we have to classify whether the generated image matches the original or not\n",
    "loss = BinaryCrossentropy(from_logits=True)\n",
    "\n",
    "# the optimizers are set to Adam with learning_rate as 2e-4 and beta_1 as 0.5\n",
    "gen_g_optimizer = gen_f_optimizer = disc_x_optimizer = disc_y_optimizer = Adam(learning_rate=2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to crop the image to our desired dimensions\n",
    "def random_crop(image):\n",
    "    image = tf.image.random_crop(image, size=[IMG_HEIGHT, IMG_WIDTH, CHANNELS])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to normalize the image to [-1, 1]\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating images with random properties such as fliping the image on vertical axis\n",
    "def random_jitter(image):\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image = random_crop(image)\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to preprocess the train images\n",
    "def preprocess_train_image(image, label):\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_elephant_image(image, label):\n",
    "    image = random_jitter(image)\n",
    "    #image = normalize(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_zebras = train_zebras.map(preprocess_train_image, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "train_elephants = train_elephants.map(preprocess_elephant_image, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# “Ck denotes a 4 × 4 Convolution-InstanceNorm-LeakyReLU layer with k filters and stride 2\"\n",
    "\n",
    "def CK(inputs, k, use_instancenorm=True):\n",
    "    block = Conv2D(k, kernel_size=(4,4), strides=2, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    if use_instancenorm:\n",
    "        block = InstanceNormalization(axis=-1)(block)\n",
    "    block = LeakyReLU(0.2)(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create discriminator with the architecture mentioned in original paper (C64, C128, C256, C512)\n",
    "def discriminator():\n",
    "\n",
    "    # declaring the shape of input image (here is it 256x256x3)\n",
    "    dis_input = Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))\n",
    "    \n",
    "    # creating the arch\n",
    "    d = CK(dis_input, 64, use_instancenorm=False)\n",
    "    d = CK(d, 128)\n",
    "    d = CK(d, 256)\n",
    "    d = CK(d, 512)\n",
    "\n",
    "    # layer to classify between the originality of generated images\n",
    "    d = Conv2D(1, kernel_size=(4,4), kernel_initializer=weight_initializer, padding='same')(d)\n",
    "\n",
    "    # generating a model using inputs and outputs\n",
    "    dis = Model(dis_input, d)\n",
    "\n",
    "    # compiling the model as it would be used to classify the images\n",
    "    dis.compile(loss='mse', optimizer=disc_x_optimizer)\n",
    "    return dis\n",
    "\n",
    "    # can change the loss function to binarycrossentropy and adding a Fully connected network as ouput layer (have to experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"c7s1-k denotes a 7×7 Convolution-InstanceNorm-ReLU with k filters and stride 1\"\n",
    "def c7s1k(inputs, k, activation):\n",
    "    block = Conv2D(k, kernel_size=(7,7), strides=1, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    block = InstanceNormalization(axis=-1)(block)\n",
    "    block = Activation(activation)(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"dk denotes a 3×3 Convolution-InstanceNorm-ReLU with k filters and stride 2\"\n",
    "def dk(inputs, k):\n",
    "    block = Conv2D(k, kernel_size=(3,3), strides=2, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    block = InstanceNormalization(axis=-1)(block)\n",
    "    block = Activation(tf.nn.relu)(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Rk denotes a residual block that contains two 3×3 convolutional layers with k filters on each layer\"\n",
    "def rk(inputs, k):\n",
    "    block = Conv2D(k, kernel_size=(3,3), strides=1, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    block = InstanceNormalization(axis=-1)(block)\n",
    "    block = Activation(tf.nn.relu)(block)\n",
    "    block = Conv2D(k, kernel_size=(3,3), strides=1, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    block = InstanceNormalization(axis=-1)(block)\n",
    "    return block + inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"uk denotes a 3×3 fractional-strided-ConvolutionInstanceNorm-ReLU layer with k filters and stride ½\" (Conv2d with 0.5 strides is Conv2dTranspose with 2 strides)\n",
    "def uk(inputs, k):\n",
    "    block = Conv2DTranspose(k, kernel_size=(3,3), strides=2, padding='same', kernel_initializer=weight_initializer)(inputs)\n",
    "    block = InstanceNormalization(axis=-1)(block)\n",
    "    block = Activation(tf.nn.relu)(block)\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method to create the generator with arch (c7s1-64, d128, d256, R256, R256, R256, R256, R256, R256, R256, R256, R256, u128, u64, c7s1-3)\n",
    "\n",
    "# note: we are not compiling the generator here as the only job of it is to generate the images and learn from the discriminator's response\n",
    " \n",
    "def generator():\n",
    "    # declaring the shape of input image (here is it 256x256x3)\n",
    "    gen_inputs = Input(shape=(IMG_HEIGHT, IMG_WIDTH, CHANNELS))\n",
    "\n",
    "    gen = c7s1k(inputs=gen_inputs, k=64, activation=tf.nn.relu)\n",
    "    gen = dk(inputs=gen, k=128)\n",
    "    gen = dk(inputs=gen, k=256)\n",
    "    \n",
    "    for _ in range(9):\n",
    "        gen = rk(inputs=gen, k=256)\n",
    "\n",
    "    gen = uk(inputs=gen, k=128)\n",
    "    gen = uk(inputs=gen, k=64)\n",
    "    \n",
    "    gen = c7s1k(inputs=gen, k=3, activation=tf.nn.tanh)\n",
    "\n",
    "    model = Model(gen_inputs, gen)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the generators and discriminators\n",
    "\n",
    "generator_g = generator()\n",
    "generator_f = generator()\n",
    "\n",
    "discriminator_x = discriminator()\n",
    "discriminator_y = discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods to find the error (loss) in data\n",
    "\n",
    "# This method is used to find how much does the generated image differ than the original \n",
    "def discriminator_loss(real, generated):\n",
    "    real_loss = loss(tf.ones_like(real), real)\n",
    "    generated_loss = loss(tf.zeros_like(generated), generated)\n",
    "    total_loss = real_loss + generated_loss\n",
    "    return total_loss * 0.5\n",
    "\n",
    "# This method is used to find the difference between generated images to true +ve \n",
    "def generator_loss(generated):\n",
    "    return loss(tf.ones_like(generated), generated)\n",
    "\n",
    "# It is the main component in CycleGAN which finds the difference between the cycled images i.e. the image generated again from the regenerated image to find out whether the input image is again generated when used in backward\n",
    "def cycle_loss(real_image, cycled_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return LAMBDA * loss\n",
    "\n",
    "# used to prevent the loss in colors and fine details\n",
    "def identity_loss(real_image, sample_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - sample_image))\n",
    "    return LAMBDA * loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring A Checkpoint so that we can start the training from where we stopped last time as we don't want to train from the scratch everytime\n",
    "\n",
    "# path to store the checkpoints\n",
    "checkpoint_path = \"../Logs/checkpoints/elephant2zebra_256\"\n",
    "\n",
    "ckpt = Checkpoint(generator_g=generator_g,\n",
    "                  generator_f=generator_f,\n",
    "                  discriminator_x=discriminator_x,\n",
    "                  discriminator_y=discriminator_y,\n",
    "                  gen_g_optimizer=gen_g_optimizer,\n",
    "                  gen_f_optimizer=gen_f_optimizer,\n",
    "            disc_x_optimizer=disc_x_optimizer,\n",
    "            disc_y_optimizer=disc_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tf.function() to compile the method before running it.\n",
    "\n",
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    # persistent is set to True because the tape is used more than\n",
    "    # once to calculate the gradients.\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "    \n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, \n",
    "                                            generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, \n",
    "                                            generator_f.trainable_variables)\n",
    "    \n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, \n",
    "                                                discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, \n",
    "                                                discriminator_y.trainable_variables)\n",
    "    \n",
    "    # Apply the gradients to the optimizer\n",
    "    gen_g_optimizer.apply_gradients(zip(generator_g_gradients, \n",
    "                                                generator_g.trainable_variables))\n",
    "    \n",
    "    gen_f_optimizer.apply_gradients(zip(generator_f_gradients, \n",
    "                                                generator_f.trainable_variables))\n",
    "    \n",
    "    disc_x_optimizer.apply_gradients(zip(discriminator_x_gradients,\n",
    "                                                    discriminator_x.trainable_variables))\n",
    "    \n",
    "    disc_y_optimizer.apply_gradients(zip(discriminator_y_gradients,\n",
    "                                                    discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A method to generate a sample image on which we can see the training efficiency visually\n",
    "\n",
    "def generate_images(model, test_image):\n",
    "    predicted = model(test_image)\n",
    "\n",
    "    plt.figure(figsize=(12,12))\n",
    "    display_list = [test_image[0], predicted[0]]\n",
    "    title = ['Test Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, 2, i+1)\n",
    "        plt.title(title[i])\n",
    "        # getting the pixel values between [0, 1] to plot it.\n",
    "        plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a samples of monet and photo to display for showcasing the training efficiency\n",
    "sample_elephant = next(iter(train_elephants))\n",
    "sample_zebra = next(iter(train_zebras))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Photo')\n",
    "plt.imshow(sample_elephant[0] * 0.5 + 0.5)\n",
    "plt.subplot(122)\n",
    "plt.title('Photo with random jitter')\n",
    "plt.imshow(random_jitter(sample_elephant[0]) * 0.5 + 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to train the GAN\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  n = 0\n",
    "  for image_x, image_y in tf.data.Dataset.zip((train_elephants, train_zebras)):\n",
    "    train_step(image_x, image_y)\n",
    "    if n % 10 == 0:\n",
    "      # used to show the progress\n",
    "      print ('.', end='')\n",
    "    n+=1\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  # Using a consistent image (sample_elephants) so that the progress of the model\n",
    "  # is clearly visible.\n",
    "  generate_images(generator_g, sample_elephant)\n",
    "\n",
    "  # defining the interval to save checkpoints\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                      time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}